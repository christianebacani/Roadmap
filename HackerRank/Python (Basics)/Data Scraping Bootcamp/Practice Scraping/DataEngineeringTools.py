# Import Libraries
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.edge.service import Service
import pandas as pd
import re 
from keybert import KeyBERT


# Funtion for Customized Web Driver
def web_driver():
    # Load an Edge Web Driver
    options = webdriver.EdgeOptions()

    # Specifying parameters for the Web Driver that is compatible to the method of Web Scraping
    options.add_argument('--verbose')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-gpu')
    options.add_argument('--disable-dev-shm-usage')
    
    # Loading the Exeutable path of Edge Web Driver
    service = Service(executable_path='D:\\Edge WebDriver Folder\\msedgedriver.exe')

    # Initializing the Parameters and Path for execution
    driver = webdriver.Edge(service=service, options=options)

    return driver

# Call the function for executing Web Scraping Process
driver = web_driver()
driver.get('https://www.kdnuggets.com/7-data-engineering-tools-for-beginners')

# Wait for the the content to be scrolled from top to bottom before executing the next functions
driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')

# Wait for the elements to be present while scrolling
WebDriverWait(driver, 30).until(
    EC.presence_of_element_located((By.XPATH, '//div[@class="post-181602 post type-post status-publish format-standard has-post-thumbnail hentry category-kdnuggets-originals tag-data-engineering"]'))
)

# Fetch all the elements
containers = driver.find_elements(By.XPATH, '//div[@class="post-181602 post type-post status-publish format-standard has-post-thumbnail hentry category-kdnuggets-originals tag-data-engineering"]')

# Array to store the data
data = []

# Parse every elements into the array
for element in containers:
    try:
        data.append(element.text) # Text format

    except Exception as e:
        print(f"Error Code : {e}")

# Combined multiple data elements in array into a single coherent string
rows = '\n'.join(data)

sentences = rows.split('\n')

# Parsed the list into dataframe
df = pd.DataFrame(sentences, columns=['Content'])

# Remove empty rows
df = df[df['Content'].str.strip() != '']
print(df)

df = df.iloc[1:20].reset_index(drop=True)

# Function for cleaning data
def clean_content(text):
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'\t+', ' ', text)
    text = re.sub(r'\n+', ' ', text)

    return text.strip() 

# Clean the Content column
df['Content'] = df['Content'].apply(clean_content)
print(df)

df.to_csv('data_engineering_tools.csv', index=False)
print("Converted into CSV File!")

df = pd.read_csv('D:\\Visual Studio Codes\\data_engineering_tools.csv')

# Initializing KeyBERT Model for Generating Keywords
nlp = KeyBERT('distilbert-base-nli-mean-tokens')

# Function for generating keywords using KeyBERT Model
def generate_keywords(data):
    # KeyBERT Function for Generating Keyword
    keywords = [keyword for keyword, score in nlp.extract_keywords(data)]

    # List Comprehension of Capitalizing every keywords generated by the model
    formatted_keywords = [word.capitalize() for word in keywords]    

    # Returned a value with Comma Separated for Proper Format
    return ', '.join(formatted_keywords)

# Execute the function
df['Category'] = df['Content'].apply(generate_keywords)
print(df)

# Save the Enriched Data into CSV File
df.to_csv('data_engineering_tools.csv', index=False)
print("Converted into CSV File!")

# Specifying the CSV Filepath for conversion
csv_filepath = 'D:\\Visual Studio Codes\\data_engineering_tools.csv'

# Convert the File Format of the CSV
textfile = csv_filepath.replace('.csv', '.txt')

# Modify the Text File to remove comma and for structuring the data
with open(textfile, 'w', encoding='utf-8', newline='\n') as text:
    # Fetch every rows in the specified column
    for i, row in df.iterrows():
        content = row.get('Content', 'No Content')
        category = row.get('Category', 'No Category')

        # Format of every rows inside the text file
        text.write(f"{i + 1}. {content}\tKeywords : '{category}'\n")

# Close the file after execution
text.close()
print("Successfully Scraped from a Website!")



